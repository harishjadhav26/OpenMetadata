apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: openmetadata-dependencies-scheduler
  name: openmetadata-dependencies-scheduler
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: openmetadata-dependencies-scheduler
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: openmetadata-dependencies-scheduler
    spec:
      affinity: {}
      containers:
      - args:
        - bash
        - -c
        - exec airflow scheduler -n -1
        command:
        - /usr/bin/dumb-init
        - --
        - /entrypoint
        env:
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: airflow-mysql-password
              name: airflow-mysql-secrets
        - name: CONNECTION_CHECK_MAX_COUNT
          value: "0"
        envFrom:
        - secretRef:
            name: openmetadata-dependencies-config-envs
        image: docker.getcollate.io/openmetadata/ingestion:1.6.5
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - /usr/bin/dumb-init
            - --
            - /entrypoint
            - python
            - -Wignore
            - -c
            - |
              import os
              import sys

              # suppress logs triggered from importing airflow packages
              os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

              # shared imports
              try:
                  from airflow.jobs.job import Job
              except ImportError:
                  # `BaseJob` was renamed to `Job` in airflow 2.6.0
                  from airflow.jobs.base_job import BaseJob as Job
              from airflow.utils.db import create_session
              from airflow.utils.net import get_hostname

              with create_session() as session:
                  ########################
                  # heartbeat check
                  ########################
                  # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                  hostname = get_hostname()
                  scheduler_job = session \
                      .query(Job) \
                      .filter_by(job_type="SchedulerJob") \
                      .filter_by(hostname=hostname) \
                      .order_by(Job.latest_heartbeat.desc()) \
                      .limit(1) \
                      .first()
                  if (scheduler_job is not None) and scheduler_job.is_alive():
                      pass
                  else:
                      sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          failureThreshold: 5
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 60
        name: airflow-scheduler
        resources: {}
        securityContext:
          runAsGroup: 0
          runAsUser: 50000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/airflow/dags
          name: dags-data
        - mountPath: /opt/airflow/logs
          name: logs-data
        - mountPath: /opt/airflow/pod_templates/pod_template.yaml
          name: pod-template
          readOnly: true
          subPath: pod_template.yaml
      dnsPolicy: ClusterFirst
      initContainers:
      - args:
        - bash
        - -c
        - exec timeout 60s airflow db check
        command:
        - /usr/bin/dumb-init
        - --
        - /entrypoint
        env:
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: airflow-mysql-password
              name: airflow-mysql-secrets
        - name: CONNECTION_CHECK_MAX_COUNT
          value: "0"
        envFrom:
        - secretRef:
            name: openmetadata-dependencies-config-envs
        image: docker.getcollate.io/openmetadata/ingestion:1.6.5
        imagePullPolicy: IfNotPresent
        name: check-db
        resources: {}
        securityContext:
          runAsGroup: 0
          runAsUser: 50000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/airflow/dags
          name: dags-data
        - mountPath: /opt/airflow/logs
          name: logs-data
      - args:
        - bash
        - -c
        - exec airflow db check-migrations -t 60
        command:
        - /usr/bin/dumb-init
        - --
        - /entrypoint
        env:
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              key: airflow-mysql-password
              name: airflow-mysql-secrets
        - name: CONNECTION_CHECK_MAX_COUNT
          value: "0"
        envFrom:
        - secretRef:
            name: openmetadata-dependencies-config-envs
        image: docker.getcollate.io/openmetadata/ingestion:1.6.5
        imagePullPolicy: IfNotPresent
        name: wait-for-db-migrations
        resources: {}
        securityContext:
          runAsGroup: 0
          runAsUser: 50000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /opt/airflow/dags
          name: dags-data
        - mountPath: /opt/airflow/logs
          name: logs-data
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 0
      serviceAccount: airflow
      serviceAccountName: airflow
      terminationGracePeriodSeconds: 30
      volumes:
      - name: dags-data
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-dags
      - name: logs-data
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-logs
      - configMap:
          defaultMode: 420
          name: openmetadata-dependencies-pod-template
        name: pod-template
